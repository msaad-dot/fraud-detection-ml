{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bb45f84-1425-414f-8e17-e141322415fd",
   "metadata": {},
   "source": [
    "# 05_Model Comparison — Credit Card Fraud Detection\n",
    "\n",
    "## Objective\n",
    "This notebook compares multiple classification models for fraud detection\n",
    "under the same preprocessing, evaluation metrics, and business-aligned\n",
    "threshold selection framework.\n",
    "\n",
    "## Persisting Final Model Artifacts\n",
    "\n",
    "To enable downstream business evaluation and cost-sensitive analysis,\n",
    "the trained models and their predicted probabilities are persisted as artifacts.\n",
    "\n",
    "Saving model outputs ensures that:\n",
    "- Cost evaluation is fully decoupled from model training\n",
    "- All models are compared under identical data and assumptions\n",
    "- Results are reproducible and production-oriented\n",
    "\n",
    "The persisted artifacts will be reused in the cost evaluation stage\n",
    "without retraining or re-running the full comparison pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e1d213-6941-49f8-a47b-1d321823fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "#loading models outputs\n",
    "baseline_outputs = joblib.load(\"../artifacts/model_outputs_baseline.pkl\")\n",
    "rf_outputs = joblib.load(\"../artifacts/model_outputs_random_forest.pkl\")\n",
    "xgb_outputs = joblib.load(\"../artifacts/model_outputs_xgboost.pkl\")\n",
    "\n",
    "y_test = baseline_outputs[\"y_test\"]\n",
    "\n",
    "y_proba_baseline = baseline_outputs[\"y_pred_proba\"]\n",
    "y_proba_rf = rf_outputs[\"y_pred_proba\"]\n",
    "y_proba_xgb = xgb_outputs[\"y_pred_proba\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736ab20c-973d-4ce9-9178-219f0a725ff5",
   "metadata": {},
   "source": [
    "## Unified Evaluation Framework\n",
    "\n",
    "To ensure a fair and unbiased comparison between different models,\n",
    "a unified evaluation function is used across all experiments.\n",
    "\n",
    "This guarantees that:\n",
    "- All models are evaluated using the same probability threshold logic\n",
    "- Precision and recall are computed consistently\n",
    "- Confusion matrices are directly comparable\n",
    "\n",
    "This approach avoids metric leakage and aligns the comparison process\n",
    "with real-world model selection practices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fab5a6-611e-49b4-af28-63ad7a5e025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_proba, threshold):\n",
    "    \"\"\"\n",
    "    Evaluate a model at a given probability threshold.\n",
    "    \"\"\"\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    \n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"confusion_matrix\": cm\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2defd165-0769-4e76-9945-921728a33781",
   "metadata": {},
   "source": [
    "## Logistic Regression — Baseline Reference\n",
    "\n",
    "The Logistic Regression model serves as a fixed baseline reference.\n",
    "Its predictions were generated during the modeling stage and are reused\n",
    "here without retraining to ensure a fair comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc80d0e-c10e-4efa-890e-1f19346bb318",
   "metadata": {},
   "source": [
    "### Baseline Performance — Threshold-Independent Evaluation\n",
    "\n",
    "Before comparing multiple models, we first establish a reference performance\n",
    "for the baseline Logistic Regression model.\n",
    "\n",
    "PR-AUC is used as a threshold-independent metric that measures the model’s\n",
    "ability to rank fraudulent transactions above normal ones. This metric is\n",
    "particularly suitable for highly imbalanced fraud detection problems and\n",
    "serves as the primary comparison criterion across all models.\n",
    "\n",
    "Baseline PR-AUC is reproduced here using stored probabilities\n",
    "to ensure consistency with other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567bbb82-6a43-4a17-803f-bfcb78816a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR-AUC for baseline Logistic Regression\n",
    "baseline_pr_auc = average_precision_score(\n",
    "    y_test,\n",
    "    y_proba_baseline\n",
    ")\n",
    "\n",
    "baseline_pr_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fd30cd-74d9-45eb-8fdc-db013caa937f",
   "metadata": {},
   "source": [
    "### Baseline Performance — Operating Threshold Evaluation\n",
    "\n",
    "While PR-AUC evaluates ranking quality, real-world fraud detection systems\n",
    "require a concrete decision threshold.\n",
    "\n",
    "The baseline model is therefore evaluated at the previously selected operating\n",
    "threshold (0.7) to quantify the trade-off between fraud recall and false alert\n",
    "volume. This establishes a practical reference point for comparing operational\n",
    "performance across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e954e6c-2a65-41b0-937c-fa54b2413b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_threshold = 0.7\n",
    "\n",
    "baseline_eval = evaluate_model(\n",
    "    y_test,\n",
    "    y_proba_baseline,\n",
    "    baseline_threshold\n",
    ")\n",
    "\n",
    "baseline_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be54f9fa-f921-459a-b127-35076b16f888",
   "metadata": {},
   "source": [
    "- For the baseline Logistic Regression model, the operating threshold was finalized\n",
    "during the standalone evaluation stage and is reused here.\n",
    "\n",
    "- For subsequent models, threshold tuning is performed within this notebook\n",
    "to identify their optimal operating points under the same evaluation framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469d5d2d-3805-4be2-b0f0-cc2daea5e654",
   "metadata": {},
   "source": [
    "### Random Forest — Threshold-Independent Evaluation (PR-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7479ddfc-2180-43c6-bcae-26874a539cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pr_auc = average_precision_score(\n",
    "    y_test,\n",
    "    y_proba_rf\n",
    ")\n",
    "\n",
    "rf_pr_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c246a9-6275-4f75-9fee-e6def4a0443d",
   "metadata": {},
   "source": [
    "### Random Forest — Operating Threshold Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db1ddde-5a14-4974-922f-bb882e8fc04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_threshold = 0.7\n",
    "\n",
    "rf_eval = evaluate_model(\n",
    "    y_test,\n",
    "    y_proba_rf,\n",
    "    rf_threshold\n",
    ")\n",
    "\n",
    "rf_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c715a3-421c-4eb9-8189-f4f88c349f5c",
   "metadata": {},
   "source": [
    "### Random Forest — Threshold Tuning\n",
    "\n",
    "To better align Random Forest with fraud detection objectives,\n",
    "multiple probability thresholds are evaluated to explore the\n",
    "precision–recall trade-off and identify a more suitable operating point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c367dc-cc57-4f80-8bfd-53538c521460",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_thresholds = np.round(np.arange(0.3, 0.81, 0.05), 2)\n",
    "\n",
    "rf_results = []\n",
    "\n",
    "for t in rf_thresholds:\n",
    "    eval_res = evaluate_model(y_test, y_proba_rf, t)\n",
    "    \n",
    "    rf_results.append({\n",
    "        \"threshold\": t,\n",
    "        \"precision\": eval_res[\"precision\"],\n",
    "        \"recall\": eval_res[\"recall\"]\n",
    "    })\n",
    "\n",
    "rf_threshold_df = pd.DataFrame(rf_results)\n",
    "rf_threshold_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2d3e15-3565-4b93-88b8-153dbf2537ad",
   "metadata": {},
   "source": [
    "### Random Forest — Threshold Selection\n",
    "\n",
    "Based on the precision–recall trade-off, a threshold of 0.35 was selected\n",
    "as the operating point for Random Forest.\n",
    "\n",
    "This threshold achieves a strong balance between fraud recall and alert\n",
    "precision, significantly reducing false positives while maintaining\n",
    "high fraud detection coverage compared to the baseline Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd8c2b4-b4f1-4410-9f63-326e4c09e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_final_threshold = 0.35\n",
    "\n",
    "# Evaluate at threshold 0.35\n",
    "rf_final_eval = evaluate_model(\n",
    "    y_test,\n",
    "    y_proba_rf,\n",
    "    rf_final_threshold\n",
    ")\n",
    "\n",
    "rf_final_eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035de8b1-7b01-4341-bd55-796ad632397b",
   "metadata": {},
   "source": [
    "## Interim Model Comparison — Logistic Regression vs Random Forest\n",
    "\n",
    "At this stage, two models have been evaluated under the same preprocessing\n",
    "and evaluation framework.\n",
    "\n",
    "The Logistic Regression model prioritizes fraud recall, successfully detecting\n",
    "most fraudulent transactions but generating a high volume of false positive alerts.\n",
    "\n",
    "In contrast, the Random Forest model demonstrates substantially stronger\n",
    "ranking performance (higher PR-AUC) and dramatically reduces false positives,\n",
    "at the cost of a moderate reduction in fraud recall.\n",
    "\n",
    "This comparison highlights the inherent trade-off between fraud detection\n",
    "coverage and customer experience, and serves as a foundation for evaluating\n",
    "more advanced models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deca2d3-f400-4af4-80e8-4744bf9e54a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_so_far = pd.DataFrame([\n",
    "    {\n",
    "        \"model\": \"Logistic Regression\",\n",
    "        \"pr_auc\": 0.716,\n",
    "        \"threshold\": 0.70,\n",
    "        \"precision\": baseline_eval[\"precision\"],\n",
    "        \"recall\": baseline_eval[\"recall\"],\n",
    "        \"false_positives\": baseline_eval[\"confusion_matrix\"][0, 1]\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Random Forest\",\n",
    "        \"pr_auc\": rf_pr_auc,\n",
    "        \"threshold\": 0.35,\n",
    "        \"precision\": rf_final_eval[\"precision\"],\n",
    "        \"recall\": rf_final_eval[\"recall\"],\n",
    "        \"false_positives\": rf_final_eval[\"confusion_matrix\"][0, 1]\n",
    "    }\n",
    "])\n",
    "\n",
    "comparison_so_far"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6422dbca-cd30-4f73-9ee6-65666442a3cc",
   "metadata": {},
   "source": [
    "## Interim Conclusion\n",
    "\n",
    "The Random Forest model substantially outperforms the Logistic Regression\n",
    "baseline in terms of ranking quality (PR-AUC) and false positive reduction.\n",
    "\n",
    "However, this improvement comes with a moderate decrease in fraud recall.\n",
    "As a result, model selection depends on business priorities:\n",
    "whether maximizing fraud detection coverage or minimizing customer disruption\n",
    "is the primary objective.\n",
    "\n",
    "This interim conclusion establishes a clear baseline for evaluating more\n",
    "advanced models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b55dcc-333a-42ae-ae33-7eec9e354f5b",
   "metadata": {},
   "source": [
    "## Gradient Boosting Model — XGBoost\n",
    "\n",
    "Gradient Boosting is evaluated to determine whether a boosted tree-based\n",
    "approach can achieve a better balance between fraud recall and false\n",
    "positive reduction compared to both Logistic Regression and Random Forest.\n",
    "\n",
    "The model is trained and evaluated under the same preprocessing and\n",
    "evaluation framework to ensure a fair comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73112db-85d7-400c-9509-38e48235acc2",
   "metadata": {},
   "source": [
    "### XGBoost — Threshold-Independent Evaluation (PR-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d793e2-1a53-48fc-a996-3f34396c3652",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pr_auc = average_precision_score(y_test, y_proba_xgb)\n",
    "xgb_pr_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8db3374-ae65-4efd-846c-11c1bf65b919",
   "metadata": {},
   "source": [
    "### XGBoost — Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ecf2a-2de9-4e45-894f-efd21d5cbf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_thresholds = np.round(np.arange(0.3, 0.81, 0.05), 2)\n",
    "\n",
    "xgb_results = []\n",
    "\n",
    "for t in xgb_thresholds:\n",
    "    eval_res = evaluate_model(y_test, y_proba_xgb, t)\n",
    "    xgb_results.append({\n",
    "        \"threshold\": t,\n",
    "        \"precision\": eval_res[\"precision\"],\n",
    "        \"recall\": eval_res[\"recall\"]\n",
    "    })\n",
    "\n",
    "xgb_threshold_df = pd.DataFrame(xgb_results)\n",
    "xgb_threshold_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9108b172-c503-414c-bcbf-115afc2ae9a8",
   "metadata": {},
   "source": [
    "### XGBoost — Selected Operating Threshold\n",
    "\n",
    "Based on the precision–recall trade-off, a threshold of 0.50 was selected\n",
    "as the operating point for XGBoost to balance fraud detection coverage\n",
    "and false alert volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0363a92-a7ce-45fd-852f-fb8afb7a45a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_final_threshold = 0.50\n",
    "\n",
    "xgb_final_eval = evaluate_model(\n",
    "    y_test,\n",
    "    y_proba_xgb,\n",
    "    xgb_final_threshold\n",
    ")\n",
    "\n",
    "xgb_final_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d08d19-fa2e-408f-8c5f-70ec09ca5bf4",
   "metadata": {},
   "source": [
    "## Final Model Comparison\n",
    "\n",
    "The following table summarizes the performance of all evaluated models\n",
    "using their selected operating thresholds.\n",
    "\n",
    "Each model is compared in terms of ranking quality (PR-AUC), fraud recall,\n",
    "precision, and false positive volume to support an informed final model\n",
    "selection decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd3c72-3ae4-4aa4-92aa-8dc34aa7823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_comparison = pd.DataFrame([\n",
    "    {\n",
    "        \"model\": \"Logistic Regression\",\n",
    "        \"pr_auc\": 0.716,\n",
    "        \"threshold\": 0.70,\n",
    "        \"precision\": baseline_eval[\"precision\"],\n",
    "        \"recall\": baseline_eval[\"recall\"],\n",
    "        \"false_positives\": baseline_eval[\"confusion_matrix\"][0, 1]\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Random Forest\",\n",
    "        \"pr_auc\": rf_pr_auc,\n",
    "        \"threshold\": 0.35,\n",
    "        \"precision\": rf_final_eval[\"precision\"],\n",
    "        \"recall\": rf_final_eval[\"recall\"],\n",
    "        \"false_positives\": rf_final_eval[\"confusion_matrix\"][0, 1]\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"XGBoost\",\n",
    "        \"pr_auc\": xgb_pr_auc,\n",
    "        \"threshold\": 0.50,\n",
    "        \"precision\": xgb_final_eval[\"precision\"],\n",
    "        \"recall\": xgb_final_eval[\"recall\"],\n",
    "        \"false_positives\": xgb_final_eval[\"confusion_matrix\"][0, 1]\n",
    "    }\n",
    "])\n",
    "\n",
    "final_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931baa33-085f-4dd1-87ec-7220022f7cef",
   "metadata": {},
   "source": [
    "## Final Model Selection\n",
    "\n",
    "Three models were evaluated under a unified preprocessing and evaluation\n",
    "framework: Logistic Regression, Random Forest, and XGBoost.\n",
    "\n",
    "Logistic Regression achieved the highest fraud recall but generated an\n",
    "excessive number of false positive alerts, making it impractical for\n",
    "real-world deployment.\n",
    "\n",
    "Random Forest significantly reduced false positives but missed a larger\n",
    "portion of fraudulent transactions.\n",
    "\n",
    "XGBoost achieved the best overall balance, delivering the highest ranking\n",
    "performance (PR-AUC), strong fraud recall, and a substantial reduction in\n",
    "false positives compared to the baseline.\n",
    "\n",
    "Based on this trade-off, XGBoost was selected as the preferred production\n",
    "candidate under a moderate risk tolerance setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca4bd63-9ffc-4126-b6b2-02f0b7f3deec",
   "metadata": {},
   "source": [
    "## Precision–Recall Curve Comparison\n",
    "\n",
    "The following plot compares the precision–recall trade-offs across all\n",
    "evaluated models and visually supports the final model selection decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e0a712-178b-476b-a731-8fd634ef3d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": y_proba_baseline,\n",
    "    \"Random Forest\": y_proba_rf,\n",
    "    \"XGBoost\": y_proba_xgb\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "for name, proba in models.items():\n",
    "    precision, recall, _ = precision_recall_curve(y_test, proba)\n",
    "    plt.plot(recall, precision, label=name)\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision–Recall Curve Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2efe13-3172-4a5f-bca7-6355860505ee",
   "metadata": {},
   "source": [
    "## Final Remarks\n",
    "\n",
    "This project demonstrates an end-to-end fraud detection pipeline,\n",
    "from baseline modeling to advanced model comparison and business-aware\n",
    "model selection.\n",
    "\n",
    "Through systematic evaluation and threshold tuning, XGBoost was selected\n",
    "as the final model due to its superior balance between fraud detection\n",
    "coverage and false positive reduction.\n",
    "\n",
    "The presented approach reflects real-world decision-making practices\n",
    "in cost-sensitive and highly imbalanced classification problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fraud-ml)",
   "language": "python",
   "name": "fraud-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
