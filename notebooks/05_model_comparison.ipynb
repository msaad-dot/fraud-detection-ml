{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bb45f84-1425-414f-8e17-e141322415fd",
   "metadata": {},
   "source": [
    "# 05_Model Comparison — Credit Card Fraud Detection\n",
    "\n",
    "## Objective\n",
    "This notebook compares multiple classification models for fraud detection\n",
    "under the same preprocessing, evaluation metrics, and business-aligned\n",
    "threshold selection framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e1d213-6941-49f8-a47b-1d321823fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acc589c-40dd-4504-ad9c-d795e9a96d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed train/test splits\n",
    "X_train = joblib.load(\"../artifacts/X_train.pkl\")\n",
    "X_test  = joblib.load(\"../artifacts/X_test.pkl\")\n",
    "y_train = joblib.load(\"../artifacts/y_train.pkl\")\n",
    "y_test  = joblib.load(\"../artifacts/y_test.pkl\")\n",
    "\n",
    "# Quick sanity check\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736ab20c-973d-4ce9-9178-219f0a725ff5",
   "metadata": {},
   "source": [
    "## Unified Evaluation Framework\n",
    "\n",
    "To ensure a fair and unbiased comparison between different models,\n",
    "a unified evaluation function is used across all experiments.\n",
    "\n",
    "This guarantees that:\n",
    "- All models are evaluated using the same probability threshold logic\n",
    "- Precision and recall are computed consistently\n",
    "- Confusion matrices are directly comparable\n",
    "\n",
    "This approach avoids metric leakage and aligns the comparison process\n",
    "with real-world model selection practices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fab5a6-611e-49b4-af28-63ad7a5e025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_proba, threshold):\n",
    "    \"\"\"\n",
    "    Evaluate a model at a given probability threshold.\n",
    "    \"\"\"\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    \n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"confusion_matrix\": cm\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2defd165-0769-4e76-9945-921728a33781",
   "metadata": {},
   "source": [
    "## Logistic Regression — Baseline Reference\n",
    "\n",
    "The previously trained Logistic Regression model is used as a baseline\n",
    "reference point for comparison.\n",
    "\n",
    "No retraining is performed in this notebook.\n",
    "Instead, the stored prediction probabilities are reused to ensure a\n",
    "fair comparison under identical evaluation conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19974f61-8960-4ce2-82ea-dd08280768c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline model outputs\n",
    "baseline_outputs = joblib.load(\"../artifacts/model_outputs_baseline.pkl\")\n",
    "\n",
    "y_test_baseline = baseline_outputs[\"y_test\"]\n",
    "y_proba_baseline = baseline_outputs[\"y_pred_proba\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc80d0e-c10e-4efa-890e-1f19346bb318",
   "metadata": {},
   "source": [
    "### Baseline Performance — Threshold-Independent Evaluation\n",
    "\n",
    "Before comparing multiple models, we first establish a reference performance\n",
    "for the baseline Logistic Regression model.\n",
    "\n",
    "PR-AUC is used as a threshold-independent metric that measures the model’s\n",
    "ability to rank fraudulent transactions above normal ones. This metric is\n",
    "particularly suitable for highly imbalanced fraud detection problems and\n",
    "serves as the primary comparison criterion across all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567bbb82-6a43-4a17-803f-bfcb78816a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR-AUC for baseline Logistic Regression\n",
    "baseline_pr_auc = average_precision_score(\n",
    "    y_test_baseline,\n",
    "    y_proba_baseline\n",
    ")\n",
    "\n",
    "baseline_pr_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fd30cd-74d9-45eb-8fdc-db013caa937f",
   "metadata": {},
   "source": [
    "### Baseline Performance — Operating Threshold Evaluation\n",
    "\n",
    "While PR-AUC evaluates ranking quality, real-world fraud detection systems\n",
    "require a concrete decision threshold.\n",
    "\n",
    "The baseline model is therefore evaluated at the previously selected operating\n",
    "threshold (0.7) to quantify the trade-off between fraud recall and false alert\n",
    "volume. This establishes a practical reference point for comparing operational\n",
    "performance across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e954e6c-2a65-41b0-937c-fa54b2413b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_threshold = 0.7\n",
    "\n",
    "baseline_eval = evaluate_model(\n",
    "    y_test_baseline,\n",
    "    y_proba_baseline,\n",
    "    baseline_threshold\n",
    ")\n",
    "\n",
    "baseline_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a098e6e-4993-4908-8993-27a383ba5f43",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "Random Forest is evaluated as a non-linear, tree-based model to assess whether\n",
    "capturing feature interactions can improve fraud detection performance over\n",
    "the linear Logistic Regression baseline.\n",
    "\n",
    "The model is trained using the same preprocessed data and evaluated using the\n",
    "same metrics and thresholding framework to ensure a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c75bad-6c66-46f3-b058-3b23d005992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd36dc-9e50-4cf7-a59d-85aecebacba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict fraud probabilities on the test set\n",
    "rf_proba = rf_model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469d5d2d-3805-4be2-b0f0-cc2daea5e654",
   "metadata": {},
   "source": [
    "### Random Forest — Threshold-Independent Evaluation (PR-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7479ddfc-2180-43c6-bcae-26874a539cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pr_auc = average_precision_score(\n",
    "    y_test,\n",
    "    rf_proba\n",
    ")\n",
    "\n",
    "rf_pr_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c246a9-6275-4f75-9fee-e6def4a0443d",
   "metadata": {},
   "source": [
    "### Random Forest — Operating Threshold Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db1ddde-5a14-4974-922f-bb882e8fc04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_threshold = 0.7\n",
    "\n",
    "rf_eval = evaluate_model(\n",
    "    y_test,\n",
    "    rf_proba,\n",
    "    rf_threshold\n",
    ")\n",
    "\n",
    "rf_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c715a3-421c-4eb9-8189-f4f88c349f5c",
   "metadata": {},
   "source": [
    "### Random Forest — Threshold Tuning\n",
    "\n",
    "To better align Random Forest with fraud detection objectives,\n",
    "multiple probability thresholds are evaluated to explore the\n",
    "precision–recall trade-off and identify a more suitable operating point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c367dc-cc57-4f80-8bfd-53538c521460",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_thresholds = np.round(np.arange(0.3, 0.81, 0.05), 2)\n",
    "\n",
    "rf_results = []\n",
    "\n",
    "for t in rf_thresholds:\n",
    "    eval_res = evaluate_model(y_test, rf_proba, t)\n",
    "    \n",
    "    rf_results.append({\n",
    "        \"threshold\": t,\n",
    "        \"precision\": eval_res[\"precision\"],\n",
    "        \"recall\": eval_res[\"recall\"]\n",
    "    })\n",
    "\n",
    "rf_threshold_df = pd.DataFrame(rf_results)\n",
    "rf_threshold_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2d3e15-3565-4b93-88b8-153dbf2537ad",
   "metadata": {},
   "source": [
    "### Random Forest — Threshold Selection\n",
    "\n",
    "Based on the precision–recall trade-off, a threshold of 0.35 was selected\n",
    "as the operating point for Random Forest.\n",
    "\n",
    "This threshold achieves a strong balance between fraud recall and alert\n",
    "precision, significantly reducing false positives while maintaining\n",
    "high fraud detection coverage compared to the baseline Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd8c2b4-b4f1-4410-9f63-326e4c09e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_final_threshold = 0.35\n",
    "\n",
    "# Evaluate at threshold 0.35\n",
    "rf_final_eval = evaluate_model(\n",
    "    y_test,\n",
    "    rf_proba,\n",
    "    rf_final_threshold\n",
    ")\n",
    "\n",
    "rf_final_eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035de8b1-7b01-4341-bd55-796ad632397b",
   "metadata": {},
   "source": [
    "## Interim Model Comparison — Logistic Regression vs Random Forest\n",
    "\n",
    "At this stage, two models have been evaluated under the same preprocessing\n",
    "and evaluation framework.\n",
    "\n",
    "The Logistic Regression model prioritizes fraud recall, successfully detecting\n",
    "most fraudulent transactions but generating a high volume of false positive alerts.\n",
    "\n",
    "In contrast, the Random Forest model demonstrates substantially stronger\n",
    "ranking performance (higher PR-AUC) and dramatically reduces false positives,\n",
    "at the cost of a moderate reduction in fraud recall.\n",
    "\n",
    "This comparison highlights the inherent trade-off between fraud detection\n",
    "coverage and customer experience, and serves as a foundation for evaluating\n",
    "more advanced models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deca2d3-f400-4af4-80e8-4744bf9e54a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_so_far = pd.DataFrame([\n",
    "    {\n",
    "        \"model\": \"Logistic Regression\",\n",
    "        \"pr_auc\": 0.716,\n",
    "        \"threshold\": 0.70,\n",
    "        \"precision\": baseline_eval[\"precision\"],\n",
    "        \"recall\": baseline_eval[\"recall\"],\n",
    "        \"false_positives\": baseline_eval[\"confusion_matrix\"][0, 1]\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Random Forest\",\n",
    "        \"pr_auc\": rf_pr_auc,\n",
    "        \"threshold\": 0.35,\n",
    "        \"precision\": rf_final_eval[\"precision\"],\n",
    "        \"recall\": rf_final_eval[\"recall\"],\n",
    "        \"false_positives\": rf_final_eval[\"confusion_matrix\"][0, 1]\n",
    "    }\n",
    "])\n",
    "\n",
    "comparison_so_far"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6422dbca-cd30-4f73-9ee6-65666442a3cc",
   "metadata": {},
   "source": [
    "## Interim Conclusion\n",
    "\n",
    "The Random Forest model substantially outperforms the Logistic Regression\n",
    "baseline in terms of ranking quality (PR-AUC) and false positive reduction.\n",
    "\n",
    "However, this improvement comes with a moderate decrease in fraud recall.\n",
    "As a result, model selection depends on business priorities:\n",
    "whether maximizing fraud detection coverage or minimizing customer disruption\n",
    "is the primary objective.\n",
    "\n",
    "This interim conclusion establishes a clear baseline for evaluating more\n",
    "advanced models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b55dcc-333a-42ae-ae33-7eec9e354f5b",
   "metadata": {},
   "source": [
    "## Gradient Boosting Model — XGBoost\n",
    "\n",
    "Gradient Boosting is evaluated to determine whether a boosted tree-based\n",
    "approach can achieve a better balance between fraud recall and false\n",
    "positive reduction compared to both Logistic Regression and Random Forest.\n",
    "\n",
    "The model is trained and evaluated under the same preprocessing and\n",
    "evaluation framework to ensure a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434d1f5f-e4d7-4a04-a0d1-1dd58b4a07cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize XGBoost model\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum(),\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc299e20-9c14-4386-a5b0-7dc78d12ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict fraud probabilities on the test set\n",
    "xgb_proba = xgb_model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73112db-85d7-400c-9509-38e48235acc2",
   "metadata": {},
   "source": [
    "### XGBoost — Threshold-Independent Evaluation (PR-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d793e2-1a53-48fc-a996-3f34396c3652",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pr_auc = average_precision_score(y_test, xgb_proba)\n",
    "xgb_pr_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8db3374-ae65-4efd-846c-11c1bf65b919",
   "metadata": {},
   "source": [
    "### XGBoost — Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ecf2a-2de9-4e45-894f-efd21d5cbf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_thresholds = np.round(np.arange(0.3, 0.81, 0.05), 2)\n",
    "\n",
    "xgb_results = []\n",
    "\n",
    "for t in xgb_thresholds:\n",
    "    eval_res = evaluate_model(y_test, xgb_proba, t)\n",
    "    xgb_results.append({\n",
    "        \"threshold\": t,\n",
    "        \"precision\": eval_res[\"precision\"],\n",
    "        \"recall\": eval_res[\"recall\"]\n",
    "    })\n",
    "\n",
    "xgb_threshold_df = pd.DataFrame(xgb_results)\n",
    "xgb_threshold_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9108b172-c503-414c-bcbf-115afc2ae9a8",
   "metadata": {},
   "source": [
    "### XGBoost — Selected Operating Threshold\n",
    "\n",
    "Based on the precision–recall trade-off, a threshold of 0.50 was selected\n",
    "as the operating point for XGBoost to balance fraud detection coverage\n",
    "and false alert volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0363a92-a7ce-45fd-852f-fb8afb7a45a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_final_threshold = 0.50\n",
    "\n",
    "xgb_final_eval = evaluate_model(\n",
    "    y_test,\n",
    "    xgb_proba,\n",
    "    xgb_final_threshold\n",
    ")\n",
    "\n",
    "xgb_final_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d08d19-fa2e-408f-8c5f-70ec09ca5bf4",
   "metadata": {},
   "source": [
    "## Final Model Comparison\n",
    "\n",
    "The following table summarizes the performance of all evaluated models\n",
    "using their selected operating thresholds.\n",
    "\n",
    "Each model is compared in terms of ranking quality (PR-AUC), fraud recall,\n",
    "precision, and false positive volume to support an informed final model\n",
    "selection decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd3c72-3ae4-4aa4-92aa-8dc34aa7823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_comparison = pd.DataFrame([\n",
    "    {\n",
    "        \"model\": \"Logistic Regression\",\n",
    "        \"pr_auc\": 0.716,\n",
    "        \"threshold\": 0.70,\n",
    "        \"precision\": baseline_eval[\"precision\"],\n",
    "        \"recall\": baseline_eval[\"recall\"],\n",
    "        \"false_positives\": baseline_eval[\"confusion_matrix\"][0, 1]\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Random Forest\",\n",
    "        \"pr_auc\": rf_pr_auc,\n",
    "        \"threshold\": 0.35,\n",
    "        \"precision\": rf_final_eval[\"precision\"],\n",
    "        \"recall\": rf_final_eval[\"recall\"],\n",
    "        \"false_positives\": rf_final_eval[\"confusion_matrix\"][0, 1]\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"XGBoost\",\n",
    "        \"pr_auc\": xgb_pr_auc,\n",
    "        \"threshold\": 0.50,\n",
    "        \"precision\": xgb_final_eval[\"precision\"],\n",
    "        \"recall\": xgb_final_eval[\"recall\"],\n",
    "        \"false_positives\": xgb_final_eval[\"confusion_matrix\"][0, 1]\n",
    "    }\n",
    "])\n",
    "\n",
    "final_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931baa33-085f-4dd1-87ec-7220022f7cef",
   "metadata": {},
   "source": [
    "## Final Model Selection\n",
    "\n",
    "Three models were evaluated under a unified preprocessing and evaluation\n",
    "framework: Logistic Regression, Random Forest, and XGBoost.\n",
    "\n",
    "Logistic Regression achieved the highest fraud recall but generated an\n",
    "excessive number of false positive alerts, making it impractical for\n",
    "real-world deployment.\n",
    "\n",
    "Random Forest significantly reduced false positives but missed a larger\n",
    "portion of fraudulent transactions.\n",
    "\n",
    "XGBoost achieved the best overall balance, delivering the highest ranking\n",
    "performance (PR-AUC), strong fraud recall, and a substantial reduction in\n",
    "false positives compared to the baseline.\n",
    "\n",
    "Based on this trade-off, XGBoost was selected as the preferred production\n",
    "candidate under a moderate risk tolerance setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca4bd63-9ffc-4126-b6b2-02f0b7f3deec",
   "metadata": {},
   "source": [
    "## Precision–Recall Curve Comparison\n",
    "\n",
    "The following plot compares the precision–recall trade-offs across all\n",
    "evaluated models and visually supports the final model selection decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e0a712-178b-476b-a731-8fd634ef3d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": y_proba_baseline,\n",
    "    \"Random Forest\": rf_proba,\n",
    "    \"XGBoost\": xgb_proba\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "for name, proba in models.items():\n",
    "    precision, recall, _ = precision_recall_curve(y_test, proba)\n",
    "    plt.plot(recall, precision, label=name)\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision–Recall Curve Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2efe13-3172-4a5f-bca7-6355860505ee",
   "metadata": {},
   "source": [
    "## Final Remarks\n",
    "\n",
    "This project demonstrates an end-to-end fraud detection pipeline,\n",
    "from baseline modeling to advanced model comparison and business-aware\n",
    "model selection.\n",
    "\n",
    "Through systematic evaluation and threshold tuning, XGBoost was selected\n",
    "as the final model due to its superior balance between fraud detection\n",
    "coverage and false positive reduction.\n",
    "\n",
    "The presented approach reflects real-world decision-making practices\n",
    "in cost-sensitive and highly imbalanced classification problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fraud-ml)",
   "language": "python",
   "name": "fraud-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
