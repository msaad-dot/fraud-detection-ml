{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "097348b7-c16f-4bef-89d3-7aca6508c4d3",
   "metadata": {},
   "source": [
    "## 03_modeling — Baseline: Logistic Regression\n",
    "\n",
    "Purpose:\n",
    "- Train baseline classification model\n",
    "- Generate raw prediction probabilities\n",
    "- Persist model and modeling outputs\n",
    "\n",
    "Notes:\n",
    "- This file performs training only.\n",
    "- Evaluation is handled in 04_evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a507547e-d695-4af7-b8c4-fdb0e1c908b2",
   "metadata": {},
   "source": [
    "### Why Logistic Regression?\n",
    "\n",
    "- Suitable for binary classification problems.\n",
    "- Works well with linearly separable data.\n",
    "- Supports `class_weight`, which is important for handling class imbalance.\n",
    "- Provides probabilistic outputs required for threshold tuning.\n",
    "- Uses the lbfgs solver for stable and efficient optimization in baseline training.\n",
    "\n",
    "**As a linear baseline model, Logistic Regression is not expected to achieve high\n",
    "precision on the fraud class due to severe class imbalance. Its primary role is to\n",
    "establish a strong, interpretable reference point for comparison with more complex models.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4bd2ba-dfdb-4e96-8cec-e0caa370353f",
   "metadata": {},
   "source": [
    "### Loading Preprocessed Data\n",
    "The model is trained using preprocessed datasets saved during the preprocessing stage.\n",
    "This ensures a clean separation between preprocessing and modeling and guarantees\n",
    "reproducibility of the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e6327e-da19-4e90-8da5-722f2c271db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "...\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load train/test splits generated during preprocessing\n",
    "X_train = joblib.load(\"../artifacts/X_train.pkl\")\n",
    "X_test  = joblib.load(\"../artifacts/X_test.pkl\")\n",
    "y_train = joblib.load(\"../artifacts/y_train.pkl\")\n",
    "y_test  = joblib.load(\"../artifacts/y_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc4353c-a624-4e02-ac1c-4e566570eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model (to be compared with future models)\n",
    "\n",
    "# Fix NumPy randomness to make results reproducible across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize Logistic Regression with class balancing\n",
    "model = LogisticRegression(\n",
    "    class_weight='balanced',  # Handle class imbalance\n",
    "    max_iter=1000,             # Ensure convergence\n",
    "    solver=\"lbfgs\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict fraud probabilities for the test set\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#save model\n",
    "model_outputs = {\n",
    "    \"model_name\": \"logistic_regression_baseline\",\n",
    "    \"y_pred_proba\": y_pred_proba,\n",
    "    \"y_test\": y_test\n",
    "}\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "os.makedirs(\"../artifacts\", exist_ok=True)\n",
    "\n",
    "joblib.dump(model, \"../models/logistic_regression_baseline.pkl\")\n",
    "joblib.dump(model_outputs,\"../artifacts/model_outputs_baseline.pkl\")\n",
    "\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6421aaa-da6a-4fa2-acec-b454aa18aa58",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "- A fixed random_state is used to ensure reproducible training results and\n",
    "consistent comparisons with future models.\n",
    "\n",
    "- Predicted probabilities represent the model’s confidence that a transaction\n",
    "is fraudulent. Using probabilities instead of hard predictions allows\n",
    "flexible threshold selection and better control over the precision–recall\n",
    "trade-off, which is critical in fraud detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9acb5b-cd6a-47f4-9e13-fa9773187074",
   "metadata": {},
   "source": [
    "### Model Outputs\n",
    "This dictionary collects all raw outputs produced by the modeling stage.\n",
    "It acts as a standardized interface between the modeling and evaluation steps.\n",
    "\n",
    "- **model_name**: Identifier for the trained model, useful when comparing multiple models.\n",
    "- **y_pred_proba**: Predicted probabilities for the positive class (fraud = 1).\n",
    "  These probabilities allow flexible threshold selection during evaluation.\n",
    "- **y_test**: Ground truth labels for the test set, required for computing evaluation metrics.\n",
    "  \n",
    "The trained model and its predicted probabilities are persisted for\n",
    "use in the evaluation and comparison stages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec53dd73-f693-4fdb-82c0-25698d407674",
   "metadata": {},
   "source": [
    "## Random Forest — Model Training\n",
    "\n",
    "Purpose:\n",
    "- Train a non-linear tree-based model\n",
    "- Capture feature interactions missed by linear models\n",
    "- Generate probability outputs for downstream evaluation\n",
    "\n",
    "While Logistic Regression provides a strong and interpretable baseline,\n",
    "its linear nature limits its ability to capture complex feature interactions.\n",
    "\n",
    "To assess whether non-linear models can improve fraud detection performance,\n",
    "tree-based and boosting models are trained next under the same preprocessing setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b077af0a-8c56-426f-a40d-210396a97365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Random Forest model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict fraud probabilities on the test set\n",
    "rf_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# save outputs\n",
    "rf_outputs = {\n",
    "    \"model_name\": \"random_forest\",\n",
    "    \"y_pred_proba\": rf_proba,\n",
    "    \"y_test\": y_test\n",
    "}\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "os.makedirs(\"../artifacts\", exist_ok=True)\n",
    "\n",
    "joblib.dump(rf_model, \"../models/random_forest.pkl\")\n",
    "joblib.dump(rf_outputs, \"../artifacts/model_outputs_random_forest.pkl\")\n",
    "\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909fa33c-0136-4fc1-afc7-fe70e86a38f6",
   "metadata": {},
   "source": [
    "## Model Outputs\n",
    "\n",
    "The trained model and its predicted probabilities are persisted for\n",
    "use in the evaluation and comparison stages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26870506-f847-47fd-9864-a2a3cd3308ae",
   "metadata": {},
   "source": [
    "## Gradient Boosting Model — XGBoost\n",
    "\n",
    "Gradient Boosting is evaluated to determine whether a boosted tree-based\n",
    "approach can achieve a better balance between fraud recall and false\n",
    "positive reduction compared to both Logistic Regression and Random Forest.\n",
    "\n",
    "The model is trained and evaluated under the same preprocessing and\n",
    "evaluation framework to ensure a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cf4348-e512-4e03-8d22-46c63152364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost model\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum(),\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict fraud probabilities on the test set\n",
    "xgb_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Save outputs\n",
    "xgb_outputs = {\n",
    "    \"model_name\": \"xgboost\",\n",
    "    \"y_pred_proba\": xgb_proba,\n",
    "    \"y_test\": y_test\n",
    "}\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "os.makedirs(\"../artifacts\", exist_ok=True)\n",
    "\n",
    "joblib.dump(xgb_model, \"../models/xgboost.pkl\")\n",
    "joblib.dump(xgb_outputs, \"../artifacts/model_outputs_xgboost.pkl\")\n",
    "\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2337655-5088-4b9f-8011-8a0c425eb78e",
   "metadata": {},
   "source": [
    "## Model Outputs\n",
    "\n",
    "The trained model and its predicted probabilities are persisted for\n",
    "use in the evaluation and comparison stages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fraud-ml)",
   "language": "python",
   "name": "fraud-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
