{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb4de688-e930-4a3d-bece-9c7954f368dd",
   "metadata": {},
   "source": [
    "## 04_evaluation — Model Performance Analysis\n",
    "\n",
    "- This notebook evaluates the baseline fraud detection model using\n",
    "probability-based metrics suitable for highly imbalanced datasets.\n",
    "- The goal is to assess the model’s decision quality and determine\n",
    "appropriate operating thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04433d1c-b6b0-45fb-b5c6-30cf6a7760d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries and loading the model outputs\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "model_outputs = joblib.load(\"../artifacts/model_outputs_baseline.pkl\")\n",
    "\n",
    "y_test = model_outputs[\"y_test\"]\n",
    "y_pred_proba = model_outputs[\"y_pred_proba\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0dd1cb-a3fc-48ff-a284-e0d134ddce1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Why Probability-Based Evaluation?\n",
    "In fraud detection, hard class predictions are insufficient due to\n",
    "severe class imbalance. Evaluating predicted probabilities allows\n",
    "flexible threshold selection and better control over precision–recall\n",
    "trade-offs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060e7a90-47da-4656-a147-bf2456342f16",
   "metadata": {},
   "source": [
    "### Overall Model Discrimination\n",
    "This section evaluates the model’s ability to discriminate between\n",
    "fraudulent and normal transactions using probability-based metrics\n",
    "that are independent of a specific classification threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7896d88d-9861-4f22-bc9e-25c71916e189",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "pr_auc = average_precision_score(y_test, y_pred_proba)\n",
    "\n",
    "roc_auc, pr_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337f3d5f-372a-4727-b68f-73a4c62ab3d0",
   "metadata": {},
   "source": [
    "- **PR-AUC** focuses on the minority (fraud) class and is more informative\n",
    "  in imbalanced settings.\n",
    "These metrics confirm that the model produces meaningful probability scores and is suitable for threshold-based decision making.\n",
    "\n",
    "- **ROC-AUC** measures the model’s ability to rank fraudulent transactions\n",
    "  above normal ones across all thresholds.\n",
    "\n",
    "These results confirm that the model produces meaningful probability\n",
    "scores and is suitable for threshold-based decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7b7082-47f0-41ee-9d4a-bcc52ed28cc6",
   "metadata": {},
   "source": [
    "### Precision–Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd5ea02-7159-4b83-99b9-50bd56680ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_vals, recall_vals, thresholds = precision_recall_curve(\n",
    "    y_test, y_pred_proba\n",
    ")\n",
    "# Plot PR curve\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall_vals, precision_vals)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision–Recall Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb64a07-ac13-4bff-a4f5-48c2ab34952d",
   "metadata": {},
   "source": [
    "The Precision–Recall curve illustrates the trade-off between detecting\n",
    "fraudulent transactions and minimizing false alerts across different\n",
    "classification thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c31a89-40c0-498d-a7a8-0e85aa72e1cc",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be75ddb-fa90-4690-978c-8932a70b84ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve points\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Compute Area Under the ROC Curve (AUC)\n",
    "roc_auc_value = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(fpr, tpr, label=f\"ROC AUC = {roc_auc_value:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")  # Random baseline\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e53376-f06c-4f1f-9861-c4509992ae08",
   "metadata": {},
   "source": [
    "The ROC curve confirms that the model achieves strong separation between\n",
    "fraudulent and normal transactions across a wide range of thresholds.\n",
    "However, precision–recall analysis remains more informative for\n",
    "threshold selection in highly imbalanced fraud detection scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a223065d-50b2-4be6-9625-3050ef02ab16",
   "metadata": {},
   "source": [
    "### Baseline Performance (Threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4149b8-4f81-4a8c-952e-56dc685331d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_05 = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# Evaluate model performance using the default threshold (0.5)\n",
    "precision_05 = precision_score(y_test, y_pred_05)\n",
    "recall_05 = recall_score(y_test, y_pred_05)\n",
    "cm_05 = confusion_matrix(y_test, y_pred_05)\n",
    "\n",
    "precision_05, recall_05, cm_05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137ab1e0-7d02-4119-8284-03078f8cbc4f",
   "metadata": {},
   "source": [
    "### Cost Considerations\n",
    "- In real-world fraud detection systems, false negatives typically incur\n",
    "a higher cost due to direct financial loss, while false positives impact\n",
    "customer experience and operational workload, threshold selection should therefore be aligned with business risk tolerance\n",
    "rather than purely statistical optimality.\n",
    "\n",
    "- The selected threshold reflects a conscious trade-off between minimizing\n",
    "missed fraud cases and controlling the volume of false alerts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f5f63c-7d30-446c-abf0-ff3a2e08186b",
   "metadata": {},
   "source": [
    "## Threshold Tuning\n",
    "The default threshold (0.5) resulted in high recall but a large number of false positives.\n",
    "To reduce unnecessary alerts while maintaining reasonable fraud detection performance, multiple threshold values will be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2155e961-8122-4324-9c75-bf6978891887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate precision-recall trade-off across multiple thresholds\n",
    "thresholds = np.round(np.arange(0.3, 0.71, 0.05), 2)\n",
    "\n",
    "results = []\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred = (y_pred_proba >= t).astype(int)\n",
    "    \n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"threshold\": t,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    })\n",
    "\n",
    "threshold_results = pd.DataFrame(results)\n",
    "threshold_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbb8688-d119-408a-92ea-fe0faf309b26",
   "metadata": {},
   "source": [
    "Based on the precision-recall trade-off, a threshold of **0.7**\n",
    "was selected as the final operating point.\n",
    "\n",
    "This threshold provides:\n",
    "- A significant reduction in false positive alerts.\n",
    "- A high fraud recall (~91%), ensuring most fraud cases are still detected.\n",
    "- A more practical balance between customer experience and fraud prevention.\n",
    "  \n",
    "**note**\n",
    "While a threshold of 0.7 was selected as the primary operating point, a slightly\n",
    "lower threshold (0.65) may also be considered in more conservative scenarios.\n",
    "This option maintains the same recall level while generating a higher volume\n",
    "of alerts, making it suitable for environments where missing fraud cases is\n",
    "significantly more costly than investigating additional false positives.\n",
    "\n",
    "The final threshold choice should therefore be aligned with business risk\n",
    "tolerance, operational capacity, and customer experience considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba79d7e-cb93-456b-9283-81243ff6ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final selected threshold\n",
    "final_threshold = 0.7\n",
    "\n",
    "# Generate final predictions\n",
    "y_final_pred = (y_pred_proba >= final_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31acb7b-4a92-4d5c-a94c-20e46e283941",
   "metadata": {},
   "source": [
    "## Confusion Matrix (Final Threshold)\n",
    "The confusion matrix summarizes the model’s performance\n",
    "using the selected threshold and highlights the balance\n",
    "between detected fraud cases and false alerts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17800654-070a-4568-b6dd-47db9cc1c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cm = confusion_matrix(y_test, y_final_pred)\n",
    "\n",
    "final_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc73837-2780-4370-b769-7d9e9044b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(\n",
    "    final_cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=[\"Normal\", \"Fraud\"],\n",
    "    yticklabels=[\"Normal\", \"Fraud\"]\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix (Threshold = 0.7)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba20910a-cf6c-45c1-9cf5-c29783cdbdf6",
   "metadata": {},
   "source": [
    "## Final Confusion Matrix Analysis\n",
    "At the selected threshold (0.7), the confusion matrix shows:\n",
    "\n",
    "- True Positives (Fraud detected): 89\n",
    "- False Negatives (Fraud missed): 9\n",
    "- False Positives (False alerts): 644\n",
    "- True Negatives (Normal transactions correctly classified): 56,220\n",
    "\n",
    "At the selected threshold:\n",
    "- The number of false positives is significantly reduced compared to the default threshold.\n",
    "- Only a small number of fraud cases are missed, which may be acceptable\n",
    "  depending on business risk tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62714ed-9f85-4770-b4e9-fd46913fd238",
   "metadata": {},
   "source": [
    "## Classification Report (Final Model)\n",
    "The classification report provides detailed performance metrics\n",
    "for both normal and fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d8b409-9ccb-45c4-8e1c-904b2ac4d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_final_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b1e70e-7d14-4a41-8f79-4b8f8ae1e943",
   "metadata": {},
   "source": [
    "## Classification Report Interpretation\n",
    "The classification report highlights the effectiveness of the selected threshold (0.7)\n",
    "in detecting fraudulent transactions.\n",
    "\n",
    "- The model achieves a high recall (~91%) for the fraud class, ensuring that most\n",
    "  fraudulent transactions are successfully detected.\n",
    "- Precision for the fraud class remains relatively low, which is expected in highly\n",
    "  imbalanced fraud detection problems and reflects a trade-off to minimize missed fraud.\n",
    "- Overall accuracy is high but not considered a primary metric due to class imbalance.\n",
    "- Threshold tuning proved essential in reducing false alarms\n",
    "  while maintaining strong fraud detection performance.\n",
    "\n",
    "This performance aligns with real-world fraud detection requirements, where detecting\n",
    "fraud is prioritized over minimizing false alerts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e68a9ba-e3d3-4d37-abcb-efe2c431c4fd",
   "metadata": {},
   "source": [
    "## Evaluation Summary\n",
    "- The baseline Logistic Regression model demonstrates strong ranking\n",
    "  capability (ROC-AUC = 0.97).\n",
    "- Threshold tuning was critical to control false positive rates.\n",
    "- A threshold of 0.7 provides a practical balance between fraud detection\n",
    "  and customer experience.\n",
    "- This baseline establishes a solid reference point for future models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fraud-ml)",
   "language": "python",
   "name": "fraud-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
